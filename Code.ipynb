{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Group SOMETHING 1D ML project\n",
    "1. Loy Pek Yong 1004475\n",
    "\n",
    "\n",
    "#Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read data\n",
    "ES_train = pd.read_csv(\"./ES/train\", sep=\" \", header=None, names=[\"word\", \"label\"], encoding=\"utf-8\", quoting=3)\n",
    "ES_dev_in = pd.read_csv(\"./ES/dev.in\", sep=\" \", header=None, names=[\"word\"], encoding=\"utf-8\", quoting=3)\n",
    "ES_dev_out = pd.read_csv(\"./ES/dev.out\", sep=\" \", header=None, names=[\"label\"], encoding=\"utf-8\", quoting=3)\n",
    "\n",
    "RU_train = pd.read_csv(\"./RU/train\", sep=\" \", header=None, names=[\"word\", \"label\"], encoding=\"utf-8\", quoting=3)\n",
    "RU_dev_in = pd.read_csv(\"./RU/dev.in\", sep=\" \", header=None, names=[\"word\"], encoding=\"utf-8\", quoting=3)\n",
    "RU_dev_out = pd.read_csv(\"./RU/dev.out\", sep=\" \", header=None, names=[\"label\"], encoding=\"utf-8\", quoting=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 Point 1\n",
    "# function that returns the emission parameters with input of training set\n",
    "\n",
    "def train_emission_param(training_set):\n",
    "# Count the number of occurrences of each label and each (label, observed value) pair\n",
    "    label_counts, emission_counts = {}, {}\n",
    "\n",
    "    for observation, label in training_set:\n",
    "        if label not in label_counts:\n",
    "            label_counts[label] = 0\n",
    "        label_counts[label] += 1\n",
    "        if (label, observation) not in emission_counts:\n",
    "            emission_counts[(label, observation)] = 0\n",
    "        emission_counts[(label, observation)] += 1\n",
    "\n",
    "# Estimate the emission parameters from the counts\n",
    "    emission_param = {}\n",
    "    for label, observation in emission_counts:\n",
    "        emission_param[(label, observation)] = emission_counts[(label, observation)] / label_counts[label]\n",
    "    return emission_param, emission_counts, label_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 Point 2\n",
    "# function that returns the test emission parameters with input of test set\n",
    "def test_emission_param(test_set, emission_param, emission_counts, label_counts):\n",
    "    # predict the label of the test set\n",
    "    k = 1\n",
    "    special_label = '#UNK#'\n",
    "    \n",
    "    for observation, label in test_set:\n",
    "\n",
    "        if (label, observation) not in emission_param:\n",
    "            # k/(count(y)+k)\n",
    "            emission_param[(special_label, observation)] = k/(label_counts[label]+k)\n",
    "            k += 1\n",
    "        \n",
    "        else:\n",
    "            #count(y-> x)/(count(y)+k)\n",
    "            emission_param[(label, observation)] = (emission_counts[(label, observation)]+k)/(label_counts[label]+k)\n",
    "\n",
    "    return emission_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-72c7d98d413b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[0mES_precision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mES_predicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mES_dev_out_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[0mES_recall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mES_predicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mES_dev_out_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m \u001b[0mES_f1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mES_precision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mES_recall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;31m# RU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-40-72c7d98d413b>\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(precision, recall)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m# calculate the f1 score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mES_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./ES/train\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Part 1 Point 3\n",
    "\n",
    "def sentiment_analysis(training_set, test_set):\n",
    "\n",
    "    special_label = '#UNK#' # special label for unknown words\n",
    "    # train the emission parameters\n",
    "    train_emission_param_data, emission_counts, label_counts = train_emission_param(training_set)\n",
    "    # test the emission parameters\n",
    "    test_emission_param_data = test_emission_param(test_set, train_emission_param_data, emission_counts, label_counts)\n",
    "\n",
    "    # predict the label of the test set\n",
    "    predicted = {}\n",
    "    for observation, label in test_set:\n",
    "        \n",
    "        # if the observation is not in the emission parameters, assign the special label\n",
    "        if (special_label, observation) in test_emission_param_data:\n",
    "            predicted[observation] = special_label\n",
    "        else:\n",
    "            # assign the label with the highest emission parameter\n",
    "            predicted[observation] = max(test_emission_param_data, key=test_emission_param_data.get)[0]\n",
    "\n",
    "def precision(predicted, test_set):\n",
    "    # calculate the precision\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if predicted:\n",
    "        for observation, label in test_set:\n",
    "            if observation in predicted and predicted[observation] == label:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    return correct/total if total > 0 else 0\n",
    "\n",
    "def recall(predicted, compare_set):\n",
    "    # calculate the recall\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if predicted:\n",
    "        for observation, label in compare_set:\n",
    "            if observation in predicted and predicted[observation] == label :\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    return correct/total if total > 0 else 0\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    # calculate the f1 score\n",
    "    return 2*precision*recall/(precision+recall)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
